{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb9d178",
   "metadata": {},
   "source": [
    "Below is a concise overview of each OpenCV/NumPy/Matplotlib function we used, followed by a high-level explanation of the full algorithm pipeline.\n",
    "\n",
    "------\n",
    "\n",
    "## Function Introductions\n",
    "\n",
    "1. **`cv2.imread(path, flag)`**\n",
    "   - **Purpose**: Load an image from disk into a NumPy array.\n",
    "   - **Common Flags**:\n",
    "     - `cv2.IMREAD_GRAYSCALE`: load as single-channel grayscale.\n",
    "     - `cv2.IMREAD_COLOR`: load as BGR color.\n",
    "2. **`cv2.threshold(src, thresh, maxval, type)`**\n",
    "   - **Purpose**: Convert a grayscale image into a binary (black/white) mask.\n",
    "   - **Parameters**:\n",
    "     - `thresh`: initial threshold value (often ignored with OTSU).\n",
    "     - `maxval`: value to assign to “foreground” pixels.\n",
    "     - `type`: e.g. `cv2.THRESH_BINARY + cv2.THRESH_OTSU` automatically picks the best threshold.\n",
    "   - **Returns**: `(ret, dst)` where `dst` is the binarized image.\n",
    "3. **`cv2.ORB_create(n_features)`**\n",
    "   - **Purpose**: Instantiate an ORB (Oriented FAST and Rotated BRIEF) feature detector + descriptor extractor.\n",
    "   - **`n_features`**: maximum number of keypoints to retain.\n",
    "4. **`orb.detectAndCompute(image, mask)`**\n",
    "   - **Purpose**:\n",
    "     1. **Detect** keypoints (corners, blobs) in the image.\n",
    "     2. **Compute** a binary descriptor for each keypoint.\n",
    "   - **Returns**:\n",
    "     - `keypoints`: list of keypoint objects (with locations, scale, orientation).\n",
    "     - `descriptors`: a NumPy array of shape `(len(keypoints), descriptor_length)`.\n",
    "5. **`cv2.BFMatcher(normType, crossCheck)`**\n",
    "   - **Purpose**: Create a “brute-force” matcher that compares every descriptor in set A against every descriptor in set B.\n",
    "   - **`normType`**: e.g. `cv2.NORM_HAMMING` for binary descriptors.\n",
    "   - **`crossCheck=True`**: only keep matches for which A→B and B→A agree.\n",
    "6. **`bf.match(des1, des2)`**\n",
    "   - **Purpose**: Find the best match in `des2` for each descriptor in `des1`.\n",
    "   - **Returns**: A list of `DMatch` objects containing `.distance` (match score) and index references.\n",
    "7. **Python built-ins**:\n",
    "   - **`sorted(iterable, key=…)`**: sort matches by ascending distance.\n",
    "   - **List slicing**: `matches[:int(len(matches)*0.3)]` takes the top 30%.\n",
    "8. **`cv2.findHomography(src_pts, dst_pts, method, ransacReprojThreshold)`**\n",
    "   - **Purpose**: Estimate a 3×3 projective transform (homography) that maps `src_pts` → `dst_pts`.\n",
    "   - **`method=cv2.RANSAC`**: uses RANSAC to reject outliers.\n",
    "   - **Returns**: `(H, mask)` where `H` is the homography matrix and `mask` indicates inlier matches.\n",
    "9. **`np.linalg.svd(A)`**\n",
    "   - **Purpose**: Singular Value Decomposition of a matrix `A = U·Σ·Vᵀ`.\n",
    "   - **Use here**: By factoring out any scaling/shear in the top-left 2×2 block of `H`, we isolate the pure rotation `R = U·Vᵀ`.\n",
    "10. **`math.atan2(y, x)`** & **`math.degrees(rad)`**\n",
    "    - **Purpose**:\n",
    "      1. `atan2(b, a)`: compute the signed angle (in radians) of the vector `[a, b]`.\n",
    "      2. `degrees(...)`: convert radians → degrees.\n",
    "11. **`cv2.warpPerspective(src, H, dsize, flags, borderValue)`**\n",
    "    - **Purpose**: Apply a full 3×3 homography `H` to warp the source image into a new perspective.\n",
    "    - **`dsize`**: output image size `(width, height)`.\n",
    "    - **`flags`**: interpolation method (e.g., `cv2.INTER_LINEAR`).\n",
    "    - **`borderValue`**: pixel value to fill outside the source image.\n",
    "12. **Matplotlib Display**\n",
    "    - **`plt.subplots(...)`**: create a grid of axes.\n",
    "    - **`ax.imshow(image, cmap='gray')`**: show a grayscale image.\n",
    "    - **`ax.set_title(...)`, `ax.axis('off')`**: annotate and hide axes.\n",
    "    - **`plt.tight_layout(); plt.show()`**: render the figure.\n",
    "\n",
    "------\n",
    "\n",
    "## Algorithm Overview\n",
    "\n",
    "1. **Preprocessing & Binarization**\n",
    "   - Convert both the **original** and **distorted** images into clean binary masks using Otsu’s threshold.\n",
    "   - This strips away noise and leaves behind only the shape silhouette.\n",
    "2. **Feature Detection & Matching**\n",
    "   - Use **ORB** to detect a few thousand keypoints on each mask, and extract their binary descriptors.\n",
    "   - Perform a **Brute-Force match** (Hamming distance) and keep the top 30% of matches by quality.\n",
    "3. **Robust Homography Estimation**\n",
    "   - Feed the matched 2D point pairs into `cv2.findHomography(..., method=RANSAC)` to compute a **3×3 homography** `H` that best explains the mapping from the distorted shape back to the original.\n",
    "   - RANSAC automatically discards mismatches and focuses on the dominant geometric transform.\n",
    "4. **Rotation Extraction**\n",
    "   - Extract the **linear part** `A = H[0:2,0:2]`.\n",
    "   - Perform **SVD** on `A` and reconstruct `R = U·Vᵀ`, which is guaranteed to be a pure rotation (no scale/shear).\n",
    "   - Compute the angle via `atan2(R[1,0], R[0,0])` and normalize it into **[0, 180°]**.\n",
    "5. **Image Alignment**\n",
    "   - Finally, apply the **full homography** `H` (not just the rotation) with `cv2.warpPerspective` to the distorted image.\n",
    "   - This undoes not only the rotation, but also any perspective skew and shear, yielding pixel-accurate alignment.\n",
    "6. **Visualization**\n",
    "   - Display the **Original**, **Distorted**, and **Aligned** images side by side to verify the quality of the registration.\n",
    "\n",
    "------\n",
    "\n",
    "This pipeline combines **feature-based matching** with **projective geometry** and a **clean linear decomposition**—making it robust to noise, partial occlusion, and mild perspective distortion, while guaranteeing an accurate rotation estimate in the desired range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load grayscale images\n",
    "img_orig = cv2.imread('demo.images/original_shape_1.png', cv2.IMREAD_GRAYSCALE)\n",
    "img_dist = cv2.imread('demo.images/distorted_shape_1.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# 2. Binarize (remove noise) so ORB focuses on the shape\n",
    "_, b_orig = cv2.threshold(img_orig, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "_, b_dist = cv2.threshold(img_dist, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "# 3. Detect ORB features on the clean masks\n",
    "orb = cv2.ORB_create(5000)\n",
    "kp1, des1 = orb.detectAndCompute(b_orig, None)\n",
    "kp2, des2 = orb.detectAndCompute(b_dist, None)\n",
    "\n",
    "# 4. Match and pick the best 30%\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
    "matches = sorted(bf.match(des1, des2), key=lambda m: m.distance)\n",
    "good = matches[: int(len(matches)*0.3)]\n",
    "\n",
    "# 5. Build point sets\n",
    "pts_orig = np.float32([kp1[m.queryIdx].pt for m in good])\n",
    "pts_dist = np.float32([kp2[m.trainIdx].pt for m in good])\n",
    "\n",
    "# 6. Estimate a homography (handles perspective + affine)\n",
    "H, mask = cv2.findHomography(pts_dist, pts_orig, cv2.RANSAC, 5.0)\n",
    "\n",
    "# 7. Decompose the top‐left 2×2 of H to get a pure rotation\n",
    "A = H[0:2, 0:2]\n",
    "# Use SVD to strip out any scaling/shear, leaving only R\n",
    "U, S, Vt = np.linalg.svd(A)\n",
    "R = U @ Vt\n",
    "# Compute angle in [0, 180)\n",
    "angle = math.degrees(math.atan2(R[1,0], R[0,0])) % 180\n",
    "print(f'Estimated rotation angle: {angle:.2f}°')\n",
    "\n",
    "# 8. Warp the distorted image back with the full homography\n",
    "h, w = img_orig.shape\n",
    "img_aligned = cv2.warpPerspective(img_dist, H, (w, h), flags=cv2.INTER_LINEAR, borderValue=0)\n",
    "\n",
    "# 9. Show the three stages side-by-side\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for a, im, title in zip(ax,\n",
    "                       [img_orig, img_dist, img_aligned],\n",
    "                       ['Original', 'Distorted', 'Aligned']):\n",
    "    a.imshow(im, cmap='gray')\n",
    "    a.set_title(title)\n",
    "    a.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_mimic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
